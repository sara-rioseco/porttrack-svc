# .github/workflows/ci-cd.yml
name: PortTrack CI/CD Pipeline

on:
  push:
    branches: [main, staging, develop]
  pull_request:
    branches: [main, staging]

permissions:
  security-events: write
  contents: read

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  # Test and Lint Job
  test:
    name: Test & Lint
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js 22.17.1
        uses: actions/setup-node@v4
        with:
          node-version: '22.17.1'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run ESLint
        run: npm run lint

      - name: Run tests
        run: npm test

      - name: Run test coverage
        run: npm run test:coverage

      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage/lcov.info
          fail_ci_if_error: false

  # Security Scan Job
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: test
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run npm audit
        run: npm audit --audit-level high

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

  # Build and Push Docker Image Job
  build:
    name: Build & Push Image
    runs-on: ubuntu-latest
    permissions:
      packages: write # Grant write permission for packages
      contents: read # Often needed for checking out code
    needs: [test, security]
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
      image-tag: ${{ steps.meta.outputs.tags }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: .
          platforms: linux/amd64,linux/arm64
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # Deploy to Staging (Rolling Update)
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/staging' || github.ref == 'refs/heads/develop'
    environment: staging
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Configure kubectl for staging
        run: |
          echo "${{ secrets.KUBE_CONFIG_STAGING }}" | base64 -d > kubeconfig
          export KUBECONFIG=kubeconfig
          kubectl config current-context

      - name: Deploy to Staging (Rolling Update)
        run: |
          export KUBECONFIG=kubeconfig
          
          # Update deployment with new image
          kubectl set image deployment/porttrack-app \
            porttrack=${{ needs.build.outputs.image-tag }} \
            -n porttrack-staging
          
          # Wait for rollout to complete
          kubectl rollout status deployment/porttrack-app \
            -n porttrack-staging \
            --timeout=300s

      - name: Run smoke tests on staging
        run: |
          export KUBECONFIG=kubeconfig
          
          # Wait for service to be ready
          kubectl wait --for=condition=available \
            deployment/porttrack-app \
            -n porttrack-staging \
            --timeout=300s
          
          # Get service URL
          STAGING_URL=$(kubectl get service porttrack-service \
            -n porttrack-staging \
            -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' || echo "localhost")
          
          # Basic health check
          if [ "$STAGING_URL" != "localhost" ]; then
            curl -f http://$STAGING_URL/health || exit 1
            curl -f http://$STAGING_URL/api/v1/status || exit 1
          else
            echo "‚ö†Ô∏è  LoadBalancer not ready, skipping external tests"
          fi

      - name: Notify Slack - Staging Success
        if: success()
        uses: 8398a7/action-slack@v3
        with:
          status: success
          text: '‚úÖ PortTrack deployed successfully to Staging'
          fields: |
            [
              {
                "title": "Branch",
                "value": "${{ github.ref_name }}",
                "short": true
              },
              {
                "title": "Commit",
                "value": "${{ github.sha }}",
                "short": true
              }
            ]
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}

  # Deploy to Production (Blue-Green)
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Configure kubectl for production
        run: |
          echo "${{ secrets.KUBE_CONFIG_PROD }}" | base64 -d > kubeconfig
          export KUBECONFIG=kubeconfig
          kubectl config current-context

      - name: Determine current active environment
        id: current-env
        run: |
          export KUBECONFIG=kubeconfig
          
          # Check which environment is currently active
          CURRENT=$(kubectl get service porttrack-service \
            -n porttrack-production \
            -o jsonpath='{.spec.selector.version}' 2>/dev/null || echo "blue")
          
          if [ "$CURRENT" = "blue" ]; then
            echo "target=green" >> $GITHUB_OUTPUT
            echo "previous=blue" >> $GITHUB_OUTPUT
          else
            echo "target=blue" >> $GITHUB_OUTPUT
            echo "previous=green" >> $GITHUB_OUTPUT
          fi
          
          echo "Current active: $CURRENT, deploying to: $([ "$CURRENT" = "blue" ] && echo "green" || echo "blue")"

      - name: Deploy to inactive environment (Blue-Green)
        run: |
          export KUBECONFIG=kubeconfig
          TARGET_ENV=${{ steps.current-env.outputs.target }}
          
          # Update the inactive environment
          kubectl set image deployment/porttrack-app-$TARGET_ENV \
            porttrack=${{ needs.build.outputs.image-tag }} \
            -n porttrack-production
          
          # Wait for deployment to be ready
          kubectl rollout status deployment/porttrack-app-$TARGET_ENV \
            -n porttrack-production \
            --timeout=600s

      - name: Run production readiness tests
        run: |
          export KUBECONFIG=kubeconfig
          TARGET_ENV=${{ steps.current-env.outputs.target }}
          
          # Port forward to test the new deployment directly
          kubectl port-forward deployment/porttrack-app-$TARGET_ENV \
            8082:8082 \
            -n porttrack-production &
          
          # Wait for port forward to be ready
          sleep 10
          
          # Health checks
          curl -f http://localhost:8082/health || exit 1
          curl -f http://localhost:8082/api/v1/status || exit 1
          curl -f http://localhost:8082/metrics || exit 1
          
          # Kill port forward
          pkill -f "kubectl port-forward" || true

      - name: Switch traffic to new environment
        run: |
          export KUBECONFIG=kubeconfig
          TARGET_ENV=${{ steps.current-env.outputs.target }}
          
          # Switch the service selector to point to new environment
          kubectl patch service porttrack-service \
            -p "{\"spec\":{\"selector\":{\"version\":\"$TARGET_ENV\"}}}" \
            -n porttrack-production
          
          echo "‚úÖ Traffic switched to $TARGET_ENV environment"

      - name: Monitor deployment health
        run: |
          export KUBECONFIG=kubeconfig
          TARGET_ENV=${{ steps.current-env.outputs.target }}
          PREVIOUS_ENV=${{ steps.current-env.outputs.previous }}
          
          # Monitor for 5 minutes
          for i in {1..10}; do
            # Get service endpoint
            SERVICE_IP=$(kubectl get service porttrack-service \
              -n porttrack-production \
              -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || \
              kubectl get service porttrack-service \
              -n porttrack-production \
              -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || \
              echo "localhost")
            
            if [ "$SERVICE_IP" != "localhost" ]; then
              HEALTH_STATUS=$(curl -s -o /dev/null -w "%{http_code}" \
                http://$SERVICE_IP/health || echo "000")
              
              if [ "$HEALTH_STATUS" != "200" ]; then
                echo "‚ùå Health check failed with status: $HEALTH_STATUS"
                echo "üîÑ Rolling back to $PREVIOUS_ENV environment"
                
                # Automatic rollback
                kubectl patch service porttrack-service \
                  -p "{\"spec\":{\"selector\":{\"version\":\"$PREVIOUS_ENV\"}}}" \
                  -n porttrack-production
                
                exit 1
              fi
              
              echo "‚úÖ Health check $i/10 passed (HTTP $HEALTH_STATUS)"
            else
              echo "‚è≥ Waiting for LoadBalancer... ($i/10)"
            fi
            
            sleep 30
          done
          
          echo "üéâ Deployment monitoring completed successfully"

      - name: Cleanup old deployment
        if: success()
        run: |
          export KUBECONFIG=kubeconfig
          PREVIOUS_ENV=${{ steps.current-env.outputs.previous }}
          
          # Scale down the previous environment (optional)
          kubectl scale deployment porttrack-app-$PREVIOUS_ENV \
            --replicas=1 \
            -n porttrack-production
          
          echo "‚úÖ Scaled down $PREVIOUS_ENV environment to 1 replica"

      - name: Notify Slack - Production Success
        if: success()
        uses: 8398a7/action-slack@v3
        with:
          status: success
          text: 'üöÄ PortTrack deployed successfully to Production (Blue-Green)'
          fields: |
            [
              {
                "title": "Environment",
                "value": "${{ steps.current-env.outputs.target }}",
                "short": true
              },
              {
                "title": "Previous",
                "value": "${{ steps.current-env.outputs.previous }}",
                "short": true
              },
              {
                "title": "Commit",
                "value": "${{ github.sha }}",
                "short": true
              },
              {
                "title": "Image",
                "value": "${{ needs.build.outputs.image-tag }}",
                "short": false
              }
            ]
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}

      - name: Notify Slack - Production Failed
        if: failure()
        uses: 8398a7/action-slack@v3
        with:
          status: failure
          text: '‚ùå PortTrack deployment to Production FAILED - Automatic rollback executed'
          fields: |
            [
              {
                "title": "Failed Environment",
                "value": "${{ steps.current-env.outputs.target }}",
                "short": true
              },
              {
                "title": "Rolled back to",
                "value": "${{ steps.current-env.outputs.previous }}",
                "short": true
              }
            ]
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}

  # Manual Rollback Job
  rollback-production:
    name: Rollback Production
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch'
    environment: production
    
    steps:
      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Configure kubectl
        run: |
          echo "${{ secrets.KUBE_CONFIG_PROD }}" | base64 -d > kubeconfig
          export KUBECONFIG=kubeconfig

      - name: Execute rollback
        run: |
          export KUBECONFIG=kubeconfig
          
          # Get current active environment
          CURRENT=$(kubectl get service porttrack-service \
            -n porttrack-production \
            -o jsonpath='{.spec.selector.version}')
          
          PREVIOUS=$([ "$CURRENT" = "blue" ] && echo "green" || echo "blue")
          
          echo "Rolling back from $CURRENT to $PREVIOUS"
          
          # Switch back to previous environment
          kubectl patch service porttrack-service \
            -p "{\"spec\":{\"selector\":{\"version\":\"$PREVIOUS\"}}}" \
            -n porttrack-production

      - name: Notify Slack - Manual Rollback
        uses: 8398a7/action-slack@v3
        with:
          status: success
          text: '‚è™ Manual rollback executed on PortTrack Production'
          fields: |
            [
              {
                "title": "Triggered by",
                "value": "${{ github.actor }}",
                "short": true
              },
              {
                "title": "Workflow",
                "value": "Manual Rollback",
                "short": true
              }
            ]
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}